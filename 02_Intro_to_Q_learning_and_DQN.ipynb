{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "colab": {
   "name": "02 Intro to Q-learning and DQN.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "6uZvpptIUy0d",
    "pP0BfD6ZUy0i",
    "CcSe5L8iUy0t",
    "pUP4wGm4Uy00",
    "Vxc09jsyUy1E",
    "lKf7xskwUy1Y",
    "DoiOo4bhUy1i"
   ]
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HYoMMvnUyzd",
    "colab_type": "text"
   },
   "source": [
    "Setup\n",
    "---\n",
    "\n",
    "Make sure to select `GPU` under Runtime > Change runtime type > Hardware accelerator!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aOC5qCkgUyzg",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import sys\n",
    "\n",
    "# Setup for use in Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Clone GitHub repository\n",
    "    !git clone https://github.com/MasterScrat/droneRL-workshop.git --single-branch\n",
    "\n",
    "    # Install packages via pip\n",
    "    !pip install -r \"droneRL-workshop/colab-requirements.txt\"\n",
    "\n",
    "    # Restart Runtime so everything takes effect\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9)\n",
    "\n",
    "    # Your Runtime will crash after this - this is normal!"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7QB-SDGR5P9y",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%cd droneRL-workshop"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hkocSoiD5RaW",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXfm856yUyzl",
    "colab_type": "text"
   },
   "source": [
    "Imports\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oCqo4ZctUyzp",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%run env/env.py\n",
    "%run helpers/rl_helpers.py\n",
    "%run agents/dqn.py\n",
    "%run agents/qlearning.py\n",
    "%run agents/random.py"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLr2aS4mUyzx",
    "colab_type": "text"
   },
   "source": [
    "Intro to Q-Learning (compass Q-table)\n",
    "---\n",
    "\n",
    "You can find a Q-learning implementation in `agents/`\n",
    "\n",
    "```\n",
    "agents/\n",
    "├── curiosity.py\n",
    "├── dqn.py\n",
    "├── logging.py\n",
    "├── qlearning.py    <-- Q-learning agent\n",
    "└── random.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AbK6d8NUUyzy",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Environment without Skyscrapers + discharge\n",
    "env = CompassQTable(DeliveryDrones())\n",
    "env.env_params.update({'n_drones': 3, 'skyscrapers_factor': 0, 'stations_factor': 0,  'discharge': 0})\n",
    "states = env.reset()\n",
    "\n",
    "print('Observation space:', env.observation_space)\n",
    "print('Initial state:', {drone_index: env.format_state(state) for drone_index, state in states.items()})\n",
    "Image.fromarray(env.render(mode='rgb_array'))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gGfDx6f2Uyz1",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Create the agents\n",
    "agents = {drone.index: RandomAgent(env) for drone in env.drones}\n",
    "agents[0] = QLearningAgent(\n",
    "    env,\n",
    "    gamma=0.95, # Discount factor\n",
    "    alpha=0.1, # Learning rate\n",
    "    # Exploration rate\n",
    "    epsilon_start=1, epsilon_decay=0.99, epsilon_end=0.01\n",
    ")\n",
    "agents"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "s813-eptUyz7",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Train agents\n",
    "trainer = MultiAgentTrainer(env, agents, reset_agents=True, seed=0)\n",
    "trainer.train(5000)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4RVKwutFUyz-",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "plot_rolling_rewards(trainer.rewards_log, drones_labels={0: 'Q-learning'})"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MzOkvRWeUy0A",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "agents[0].get_qtable()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BIEipSHPUy0E",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "plt.plot(agents[0].gamma**np.arange(100))\n",
    "plt.title('Discount factor: {}'.format(agents[0].gamma))\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Discount')\n",
    "plt.show()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Dm2xVZruUy0H",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "rewards_log = test_agents(env, agents, n_steps=1000, seed=0)\n",
    "plot_cumulative_rewards(rewards_log, drones_labels={0: 'Q-learning'})"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3GONoe76Uy0K",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "path = os.path.join('output', 'videos', 'ql-compass.mp4')\n",
    "render_video(env, agents, video_path=path, n_steps=120, fps=1, seed=0)\n",
    "ColabVideo(path)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDqhulIqUy0N",
    "colab_type": "text"
   },
   "source": [
    "Scaling Q-learning (compass + lidar Q-table)\n",
    "---\n",
    "\n",
    "Let's see how Q-learning scales to larger observation spaces"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "izDZjq0dUy0O",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Environment with skyscrapers but without discharge\n",
    "env = LidarCompassQTable(DeliveryDrones())\n",
    "env.env_params.update({'n_drones': 3, 'skyscrapers_factor': 3, 'stations_factor': 0, 'discharge': 0})\n",
    "states = env.reset()\n",
    "\n",
    "print('Observation space:', env.observation_space)\n",
    "print('Sample state:', {drone_index: env.format_state(state) for drone_index, state in states.items()})\n",
    "Image.fromarray(env.render(mode='rgb_array'))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L79FTJ_XUy0R",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Create the agents\n",
    "agents = {drone.index: RandomAgent(env) for drone in env.drones}\n",
    "agents[0] = QLearningAgent(\n",
    "    env,\n",
    "    gamma=0.95, # Discount factor\n",
    "    alpha=0.1, # Learning rate\n",
    "    # Exploration rate\n",
    "    epsilon_start=1, epsilon_decay=0.99, epsilon_end=0.01\n",
    ")\n",
    "agents"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wRaa4VTaUy0T",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Train agents\n",
    "trainer = MultiAgentTrainer(env, agents, reset_agents=True, seed=0)\n",
    "trainer.train(5000)\n",
    "plot_rolling_rewards(trainer.rewards_log, drones_labels={0: 'Q-learning'})"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2yGIpaa0Uy0X",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "rewards_log = test_agents(env, agents, n_steps=1000, seed=0)\n",
    "plot_cumulative_rewards(rewards_log, drones_labels={0: 'Q-learning'})"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WLNOPGY-Uy0a",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "path = os.path.join('output', 'videos', 'ql-compass-lidar-1st-try.mp4')\n",
    "render_video(env, agents, video_path=path, n_steps=120, fps=1, seed=0)\n",
    "ColabVideo(path)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uZvpptIUy0d",
    "colab_type": "text"
   },
   "source": [
    "Issues with Q-learning\n",
    "---\n",
    "\n",
    "Two issues here\n",
    "\n",
    "* Sparse reward: pickup rate is around 1%\n",
    "* No generalization: need to explore entire space!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1nYMS72eUy0d",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "q_table = agents[0].get_qtable()\n",
    "print('Q-table:', q_table.shape)\n",
    "q_table.sample(10)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KaRv4FJBUy0g",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "plt.plot(agents[0].epsilons)\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.ylabel('Exploration rate (epsilon)')\n",
    "plt.show()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pP0BfD6ZUy0i",
    "colab_type": "text"
   },
   "source": [
    "Possible solutions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TXvcNBJHUy0j",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# (1/2) Sparse rewards: Create an intermediate \"pickup\" reward\n",
    "env.env_params.update({\n",
    "    'n_drones': 3, 'pickup_reward': 0.99, 'delivery_reward': 1,\n",
    "    'skyscrapers_factor': 3, 'stations_factor': 0, 'discharge': 0})\n",
    "states = env.reset()\n",
    "\n",
    "# (2/2) Train longer ..\n",
    "agents[0].epsilon = 1\n",
    "agents[0].epsilon_decay = 0.999\n",
    "\n",
    "set_seed(env, seed=0) # Make things deterministic\n",
    "trainer.train(30000)\n",
    "\n",
    "plot_rolling_rewards(\n",
    "    trainer.rewards_log,\n",
    "    events={'pickup': [0.99], 'delivery': [1], 'crash': [-1]},\n",
    "    drones_labels={0: 'Q-learning'})"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D9-Di0UdUy0n",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "plt.plot(agents[0].epsilons)\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.ylabel('Exploration rate (epsilon)')\n",
    "plt.show()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kGn--9h0Uy0q",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "rewards_log = test_agents(env, agents, n_steps=1000, seed=0)\n",
    "plot_cumulative_rewards(\n",
    "    rewards_log,\n",
    "    events={'pickup': [0.99], 'delivery': [1], 'crash': [-1]},\n",
    "    drones_labels={0: 'Q-learning'}\n",
    ")"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcSe5L8iUy0t",
    "colab_type": "text"
   },
   "source": [
    "Overfitting issues: try with different seeds\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YQA2LqdOUy0t",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "rewards_log = test_agents(env, agents, n_steps=1000, seed=1)\n",
    "plot_cumulative_rewards(\n",
    "    rewards_log,\n",
    "    events={'pickup': [0.99], 'delivery': [1], 'crash': [-1]},\n",
    "    drones_labels={0: 'Q-learning'}\n",
    ")"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22FXd1wWUy0w",
    "colab_type": "text"
   },
   "source": [
    "Pick a good seed for your video ;)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pdiIPpN_Uy0x",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "path = os.path.join('output', 'videos', 'ql-compass-lidar-2nd-try.mp4')\n",
    "render_video(env, agents, video_path=path, n_steps=120, fps=1, seed=1)\n",
    "ColabVideo(path)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUP4wGm4Uy00",
    "colab_type": "text"
   },
   "source": [
    "Q-learning limitations: discrete Q-table!\n",
    "---\n",
    "\n",
    "Let's try Q-learning with the full environment: skyscrapers + charge"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f2bo-ZfoUy01",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "env = LidarCompassChargeQTable(DeliveryDrones())\n",
    "env.env_params.update({\n",
    "    'n_drones': 3, 'pickup_reward': 0.99, 'delivery_reward': 1,\n",
    "    'discharge': 10, 'charge': 20, 'charge_reward': -0.1  # (default values)\n",
    "})\n",
    "states = env.reset()\n",
    "\n",
    "print('Observation space:', env.observation_space)\n",
    "print('Sample state:', env.format_state(states[0]))\n",
    "Image.fromarray(env.render(mode='rgb_array'))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AZ9nORBUUy03",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Create the agents\n",
    "agents = {drone.index: RandomAgent(env) for drone in env.drones}\n",
    "agents[0] = QLearningAgent(\n",
    "    env, gamma=0.95, alpha=0.1,\n",
    "    epsilon_start=1, epsilon_decay=0.999, epsilon_end=0.01\n",
    ")\n",
    "\n",
    "trainer = MultiAgentTrainer(env, agents, reset_agents=True, seed=0)\n",
    "trainer.train(35000)\n",
    "plot_rolling_rewards(trainer.rewards_log, events={'pickup': [0.99], 'delivery': [1], 'crash': [-1], 'charging': [-0.1]})"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rFLqZBmMUy06",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "q_table = agents[0].get_qtable()\n",
    "print('Q-table:', q_table.shape)\n",
    "q_table.sample(10)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QT-1YyNQUy09",
    "colab_type": "text"
   },
   "source": [
    "Don't forget to test with different seeds"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DvCnWPFSUy0-",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "rewards_log = test_agents(env, agents, n_steps=1000, seed=0)\n",
    "plot_cumulative_rewards(\n",
    "    rewards_log,\n",
    "    events={'pickup': [0.99], 'delivery': [1], 'crash': [-1], 'charging': [-0.1]},\n",
    "    drones_labels={0: 'Q-learning'}\n",
    ")"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NeFqgVd1Uy1B",
    "colab_type": "text"
   },
   "source": [
    "This is also related to the implementation of our **Trainer class** which **resets the environment only once** at the beginning, before training. Resetting the environment every X steps would help, but won't solve the important limitations with Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4Jg68dr9Uy1B",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "path = os.path.join('output', 'videos', 'ql-compass-lidar-charge.mp4')\n",
    "render_video(env, agents, video_path=path, n_steps=120, fps=1, seed=0)\n",
    "ColabVideo(path)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vxc09jsyUy1E",
    "colab_type": "text"
   },
   "source": [
    "First tests with deep Q-learning (DQN)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7-gdJaRNUy1F",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Create environment\n",
    "env = LidarCompassChargeQTable(DeliveryDrones())\n",
    "env.env_params.update({\n",
    "    'n_drones': 3, 'pickup_reward': 0.99, 'delivery_reward': 1\n",
    "})\n",
    "states = env.reset()\n",
    "\n",
    "# Create the agents\n",
    "agents = {drone.index: RandomAgent(env) for drone in env.drones}\n",
    "agents[0] = DQNAgent(\n",
    "    env, DenseQNetworkFactory(env, hidden_layers=[256, 256]),\n",
    "    gamma=0.95, epsilon_start=1, epsilon_decay=0.999, epsilon_end=0.01,\n",
    "    memory_size=10000, batch_size=64, target_update_interval=5\n",
    ")\n",
    "trainer = MultiAgentTrainer(env, agents, reset_agents=True, seed=0)\n",
    "agents[0].qnetwork"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pHRN104hUy1J",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Train the agents\n",
    "trainer.train(25000)\n",
    "plot_rolling_rewards(\n",
    "    trainer.rewards_log, drones_labels={0: 'DQN'},\n",
    "    events={'pickup': [0.99], 'delivery': [1], 'crash': [-1], 'charging': [-0.1]})"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1xPZdEipUy1L",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "plt.plot(agents[0].epsilons)\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.ylabel('Exploration rate (epsilon)')\n",
    "plt.show()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRcXMtLgUy1Q",
    "colab_type": "text"
   },
   "source": [
    "Try with different seeds"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uGKesCiKUy1S",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "rewards_log = test_agents(env, agents, n_steps=1000, seed=0)\n",
    "plot_cumulative_rewards(\n",
    "    rewards_log, drones_labels={0: 'DQN'},\n",
    "    events={'pickup': [0.99], 'delivery': [1], 'crash': [-1], 'charging': [-0.1]})"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "enf8PZEZUy1U",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Inspect replay memory buffer\n",
    "agents[0].inspect_memory(top_n=10, max_col=80)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omlSDb-9Uy1W",
    "colab_type": "text"
   },
   "source": [
    "Take a moment to play with the different parameters: `memory_size`, `batch_size`, `target_update_interval`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BklaA8CpUy1X",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "path = os.path.join('output', 'videos', 'dqn-compass-lidar-charge.mp4')\n",
    "render_video(env, agents, video_path=path, n_steps=120, fps=1, seed=0)\n",
    "ColabVideo(path)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKf7xskwUy1Y",
    "colab_type": "text"
   },
   "source": [
    "DQN and WindowedGrid\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DZhqD-kkUy1Z",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Create environment\n",
    "env = WindowedGridView(DeliveryDrones(), radius=3)\n",
    "env.env_params.update({\n",
    "    'n_drones': 3, 'pickup_reward': 0.99, 'delivery_reward': 1\n",
    "})\n",
    "states = env.reset()\n",
    "\n",
    "# Create the agents\n",
    "agents = {drone.index: RandomAgent(env) for drone in env.drones}\n",
    "agents[0] = my_agent = DQNAgent(\n",
    "    env, ConvQNetworkFactory(env, conv_layers=[\n",
    "        {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "        {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "        {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "        {'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "        {'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "        {'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    ], dense_layers=[1024, 256]),\n",
    "    gamma=0.95, epsilon_start=1, epsilon_decay=0.99, epsilon_end=0.01,\n",
    "    memory_size=10000, batch_size=64, target_update_interval=5\n",
    ")\n",
    "trainer = MultiAgentTrainer(env, agents, reset_agents=True, seed=0)\n",
    "agents[0].qnetwork"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Fiqt2dVBUy1b",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Train the agents\n",
    "for run in range(10):\n",
    "  trainer.train(2500)\n",
    "  plot_rolling_rewards(\n",
    "      trainer.rewards_log, drones_labels={0: 'DQN'},\n",
    "      events={'pickup': [0.99], 'delivery': [1], 'crash': [-1], 'charging': [-0.1]})"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CbjGOe3_Uy1e",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "path = os.path.join('output', 'videos', 'dqn-windowed.mp4')\n",
    "render_video(env, agents, video_path=path, n_steps=120, fps=1, seed=0)\n",
    "ColabVideo(path)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoiOo4bhUy1i",
    "colab_type": "text"
   },
   "source": [
    "Share your agent q-network\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SATrBaezUy1j",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "path = os.path.join('output', 'agents', 'dqn-agent.pt')\n",
    "agents[0].save(path)\n",
    "# agents[0].load(path) # Later, load the qnetwork!"
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}